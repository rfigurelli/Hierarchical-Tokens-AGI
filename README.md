# Hierarchical Tokens: Structuring Transformers for AGI

This repository contains the original white paper proposing a novel architectural direction for transformer-based AI models. Instead of relying solely on token-level prediction (word by word), this approach introduces **hierarchical tokenization**, treating higher-level language unitsâ€”such as sentences, paragraphs, sections, and even chaptersâ€”as "macro-tokens" within a structured predictive system.

The core idea is simple: apply the same mechanism used to predict the next word, but at multiple levels of abstraction. By organizing language generation hierarchically, this method enables more coherent reasoning, intentionality, and narrative consistencyâ€”paving the way toward **Artificial General Intelligence (AGI)**.

## ðŸ§  Core Concepts
- Predicting not just the next word, but the next sentence, paragraph, or idea
- Treating higher-level units as tokens in their own right
- Applying attention, embeddings, and sampling at each level
- Moving from token completion to **thought structuring**
- Bridging the gap between probabilistic language and structured cognition

## ðŸ“„ Files
- `hierarchical_tokens_AGI.pdf` â€“ The white paper outlining the concept and rationale

## ðŸ‘¤ Author
RogÃ©rio Figurelli  
April 25, 2025

## ðŸ“œ License
This work is licensed under the MIT License.
